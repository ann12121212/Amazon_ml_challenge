{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13331391,"sourceType":"datasetVersion","datasetId":8452338}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:32:35.847355Z","iopub.execute_input":"2025-10-13T10:32:35.847548Z","iopub.status.idle":"2025-10-13T10:32:50.627689Z","shell.execute_reply.started":"2025-10-13T10:32:35.847522Z","shell.execute_reply":"2025-10-13T10:32:50.626796Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.35.3 tokenizers-0.22.1 transformers-4.57.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Local Inference on GPU \nModel page: https://huggingface.co/microsoft/deberta-v3-base\n\n‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/microsoft/deberta-v3-base)\n\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"fill-mask\", model=\"microsoft/deberta-v3-base\")","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:32:50.631642Z","iopub.execute_input":"2025-10-13T10:32:50.631904Z","iopub.status.idle":"2025-10-13T10:33:19.863550Z","shell.execute_reply.started":"2025-10-13T10:32:50.631881Z","shell.execute_reply":"2025-10-13T10:33:19.862993Z"}},"outputs":[{"name":"stderr","text":"2025-10-13 10:32:59.187105: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760351579.351471      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760351579.399774      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d745cdc4e1934e2899bfdde19952f58e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91847a5762c744faa594463d485de31b"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06ce8657556e4e5da3eb9d127bc07984"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eb6134b65c7467baa1e17ae21775d49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7ba53f69450470794abe65e9d95ec96"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nDevice set to use cuda:0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\", dtype=\"auto\")","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:35:57.419986Z","iopub.execute_input":"2025-10-13T10:35:57.420248Z","iopub.status.idle":"2025-10-13T10:35:58.132424Z","shell.execute_reply.started":"2025-10-13T10:35:57.420229Z","shell.execute_reply":"2025-10-13T10:35:58.131617Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Remote Inference via Inference Providers \nEnsure you have a valid **HF_TOKEN** set in your environment. You can get your token from [your settings page](https://huggingface.co/settings/tokens). Note: running this may incur charges above the free tier.\nThe following Python example shows how to run the model remotely on HF Inference Providers, automatically selecting an available inference provider for you. \nFor more information on how to use the Inference Providers, please refer to our [documentation and guides](https://huggingface.co/docs/inference-providers/en/index).","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"import os\nos.environ['HF_TOKEN'] = '/'","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:37:07.685383Z","iopub.execute_input":"2025-10-13T10:37:07.685669Z","iopub.status.idle":"2025-10-13T10:37:07.689863Z","shell.execute_reply.started":"2025-10-13T10:37:07.685648Z","shell.execute_reply":"2025-10-13T10:37:07.688915Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# =====================================\n# MINIMAL FIXED VERSION WITH VALIDATION\n# 60K TRAIN + 15K VALIDATION - FIXED\n# =====================================\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\nimport re\nimport gc\nimport os\nimport joblib\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# =====================================\n# FIX: Replace lambda with proper function\n# =====================================\n\nclass TargetTransformer:\n    def __init__(self):\n        self.scaler = RobustScaler()\n        self.is_fitted = False\n    \n    def fit_transform(self, targets):\n        targets = np.array(targets)\n        log_targets = np.log1p(targets)\n        scaled = self.scaler.fit_transform(log_targets.reshape(-1, 1)).flatten()\n        self.is_fitted = True\n        return scaled\n    \n    def transform(self, targets):  # ‚úÖ ADDED MISSING METHOD\n        if not self.is_fitted:\n            raise ValueError(\"Transformer not fitted\")\n        targets = np.array(targets)\n        log_targets = np.log1p(targets)\n        return self.scaler.transform(log_targets.reshape(-1, 1)).flatten()\n    \n    def inverse_transform(self, scaled_targets):\n        if not self.is_fitted:\n            raise ValueError(\"Transformer not fitted\")\n        log_targets = self.scaler.inverse_transform(scaled_targets.reshape(-1, 1)).flatten()\n        return np.expm1(log_targets)\n\n# =====================================\n# SMAPE CALCULATION FOR VALIDATION\n# =====================================\n\ndef calculate_smape(preds, targets):\n    \"\"\"Calculate SMAPE metric for validation\"\"\"\n    return 100 * np.mean(2 * np.abs(preds - targets) / (np.abs(preds) + np.abs(targets) + 1e-8))\n\n# =====================================\n# ORIGINAL TEXT PROCESSING (WORKING)\n# =====================================\n\ndef fast_text_processing(text):\n    \"\"\"Fast but effective text processing\"\"\"\n    if pd.isna(text):\n        return \"No description\"\n    \n    text = str(text)\n    parts = []\n    \n    # 1. Product Name (CRITICAL)\n    name_match = re.search(r'Item Name:\\s*([^\\n]+)', text, re.IGNORECASE)\n    if name_match:\n        parts.append(f\"PRODUCT: {name_match.group(1).strip()}\")\n    \n    # 2. Quantity & Unit (CRITICAL)\n    value_match = re.search(r'Value:\\s*([\\d.]+)', text, re.IGNORECASE)\n    unit_match = re.search(r'Unit:\\s*([^\\n]+)', text, re.IGNORECASE)\n    if value_match and unit_match:\n        parts.append(f\"SIZE: {value_match.group(1)} {unit_match.group(1)}\")\n    \n    # 3. First 3 Bullet Points (IMPORTANT)\n    bullets = re.findall(r'Bullet Point \\d+:\\s*([^\\n]+)', text)[:3]\n    for bullet in bullets:\n        parts.append(f\"FEAT: {bullet.strip()}\")\n    \n    # Fallback\n    if not parts:\n        text = re.sub(r'\\s+', ' ', text)\n        parts.append(text[:300])\n    \n    return \" | \".join(parts)\n\n# =====================================\n# ORIGINAL FEATURE ENGINEERING (WORKING)\n# =====================================\n\ndef extract_essential_features(df):\n    \"\"\"Only the most important features\"\"\"\n    features = []\n    \n    for text in df['catalog_content']:\n        text = str(text)\n        feature_row = []\n        \n        # 1. EXTRACTED VALUE (MOST IMPORTANT)\n        value_match = re.search(r'Value:\\s*([\\d.]+)', text, re.IGNORECASE)\n        value = float(value_match.group(1)) if value_match else 0.0\n        feature_row.append(value)\n        \n        # 2. UNIT TYPE (VERY IMPORTANT)\n        unit_match = re.search(r'Unit:\\s*([^\\n]+)', text, re.IGNORECASE)\n        unit_text = unit_match.group(1).lower() if unit_match else \"\"\n        \n        # Only essential unit types\n        unit_ounce = 1 if any(word in unit_text for word in ['ounce', 'oz']) else 0\n        unit_count = 1 if any(word in unit_text for word in ['count', 'ct', 'piece']) else 0\n        unit_fluid = 1 if any(word in unit_text for word in ['fluid', 'fl']) else 0\n        \n        feature_row.extend([unit_ounce, unit_count, unit_fluid])\n        \n        # 3. TEXT LENGTH (IMPORTANT)\n        feature_row.append(len(text))\n        feature_row.append(len(re.findall(r'Bullet Point \\d+:', text)))\n        \n        features.append(feature_row)\n    \n    return np.array(features)\n\n# =====================================\n# ORIGINAL MODEL (WORKING)\n# =====================================\n\nclass FastDebertaPredictor(nn.Module):\n    def __init__(self, model_name=\"microsoft/deberta-v3-base\", n_features=6, dropout=0.3):\n        super().__init__()\n        \n        # DeBERTa v3 base\n        self.deberta = AutoModel.from_pretrained(model_name)\n        \n        # ORIGINAL FINE-TUNING: Unfreeze last 4 layers\n        for param in self.deberta.parameters():\n            param.requires_grad = False\n            \n        for layer in self.deberta.encoder.layer[-4:]:\n            for param in layer.parameters():\n                param.requires_grad = True\n        \n        # Efficient regressor\n        self.regressor = nn.Sequential(\n            nn.Linear(768 + n_features, 384),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            \n            nn.Linear(384, 192),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            \n            nn.Linear(192, 96),\n            nn.ReLU(),\n            \n            nn.Linear(96, 1)\n        )\n    \n    def forward(self, input_ids, attention_mask, features):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        cls_embedding = outputs.last_hidden_state[:, 0, :]\n        combined = torch.cat([cls_embedding, features], dim=1)\n        return self.regressor(combined).squeeze()\n\n# =====================================\n# ORIGINAL DATASET (WORKING)\n# =====================================\n\nclass FastDataset(Dataset):\n    def __init__(self, texts, features, targets, tokenizer, max_len=192):\n        self.texts = texts\n        self.features = features\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        features = self.features[idx]\n        \n        encoding = self.tokenizer(\n            text,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        \n        item = {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'features': torch.tensor(features, dtype=torch.float32)\n        }\n        \n        if self.targets is not None:\n            item['labels'] = torch.tensor(self.targets[idx], dtype=torch.float32)\n        \n        return item\n\n# =====================================\n# FIXED TRAINING FUNCTION WITH VALIDATION\n# =====================================\n\ndef train_with_validation(train_df, val_df, n_epochs=3):\n    \"\"\"Training with proper validation split\"\"\"\n    \n    print(\"üöÄ TRAINING WITH VALIDATION - 60K TRAIN / 15K VAL\")\n    print(f\"üìä Train samples: {len(train_df)}, Val samples: {len(val_df)}\")\n    \n    # Load tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n    model = FastDebertaPredictor(n_features=6).to(device)\n    \n    # Process TRAIN data\n    print(\"üìù Processing train texts...\")\n    train_texts = train_df['catalog_content'].apply(fast_text_processing).tolist()\n    \n    print(\"üîß Engineering train features...\")\n    train_features = extract_essential_features(train_df)\n    \n    # Scale features (FIT ONLY ON TRAIN)\n    feature_scaler = RobustScaler()\n    train_features_scaled = feature_scaler.fit_transform(train_features)\n    \n    # Transform targets (FIT ONLY ON TRAIN)\n    target_transformer = TargetTransformer()\n    train_targets_transformed = target_transformer.fit_transform(train_df['price'].values)\n    \n    # Process VALIDATION data (TRANSFORM ONLY)\n    print(\"üìù Processing validation texts...\")\n    val_texts = val_df['catalog_content'].apply(fast_text_processing).tolist()\n    \n    print(\"üîß Engineering validation features...\")\n    val_features = extract_essential_features(val_df)\n    val_features_scaled = feature_scaler.transform(val_features)  # Transform, not fit\n    \n    val_targets_transformed = target_transformer.transform(val_df['price'].values)  # ‚úÖ NOW WORKS!\n    val_targets_original = val_df['price'].values  # Keep original for SMAPE calculation\n    \n    # Create dataloaders\n    train_dataset = FastDataset(train_texts, train_features_scaled, train_targets_transformed, tokenizer)\n    val_dataset = FastDataset(val_texts, val_features_scaled, val_targets_transformed, tokenizer)\n    \n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n    \n    # Optimizer\n    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    \n    # Loss function\n    criterion = nn.HuberLoss()\n    \n    # Learning rate scheduler\n    total_steps = len(train_loader) * n_epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(0.1 * total_steps),\n        num_training_steps=total_steps\n    )\n    \n    print(f\"üöÄ Training for {n_epochs} epochs with validation\")\n    \n    best_val_smape = float('inf')\n    best_epoch = 0\n    \n    for epoch in range(n_epochs):\n        print(f\"\\nüéØ EPOCH {epoch+1}/{n_epochs}\")\n        \n        # ========== TRAINING ==========\n        model.train()\n        epoch_losses = []\n        \n        for batch_idx, batch in enumerate(train_loader):\n            optimizer.zero_grad()\n            \n            # Move to device\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            # Forward + backward\n            outputs = model(\n                input_ids=batch['input_ids'],\n                attention_mask=batch['attention_mask'], \n                features=batch['features']\n            )\n            \n            loss = criterion(outputs, batch['labels'])\n            loss.backward()\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            epoch_losses.append(loss.item())\n            \n            if batch_idx % 150 == 0:\n                print(f\"   Batch {batch_idx:4d} | Loss: {loss.item():.4f}\")\n        \n        avg_train_loss = np.mean(epoch_losses)\n        \n        # ========== VALIDATION ==========\n        model.eval()\n        val_preds = []\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                outputs = model(\n                    input_ids=batch['input_ids'],\n                    attention_mask=batch['attention_mask'],\n                    features=batch['features']\n                )\n                val_preds.extend(outputs.cpu().numpy())\n        \n        # Convert validation predictions back to original scale\n        val_preds_original = target_transformer.inverse_transform(np.array(val_preds))\n        \n        # Calculate validation SMAPE\n        val_smape = calculate_smape(val_preds_original, val_targets_original)\n        \n        print(f\"üìä Epoch {epoch+1} Results:\")\n        print(f\"   Train Loss: {avg_train_loss:.4f}\")\n        print(f\"   Val SMAPE: {val_smape:.4f}%\")\n        \n        # Save best model based on validation SMAPE\n        if val_smape < best_val_smape:\n            best_val_smape = val_smape\n            best_epoch = epoch + 1\n            \n            # Save model weights\n            torch.save(model.state_dict(), \"best_model_weights.pt\")\n            \n            # Save scalers using joblib\n            joblib.dump(feature_scaler, \"feature_scaler.pkl\")\n            joblib.dump(target_transformer.scaler, \"target_scaler.pkl\")\n            \n            print(f\"‚úÖ NEW BEST MODEL! Val SMAPE: {best_val_smape:.4f}%\")\n        \n        # Save checkpoint\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'val_smape': val_smape,\n        }, f\"checkpoint_epoch_{epoch+1}.pt\")\n        \n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    print(f\"\\nüèÜ TRAINING COMPLETE!\")\n    print(f\"   Best Val SMAPE: {best_val_smape:.4f}% (Epoch {best_epoch})\")\n    \n    return model, feature_scaler, target_transformer, best_val_smape\n\n# =====================================\n# FIXED PREDICTION FUNCTION\n# =====================================\n\ndef predict_fast_deberta(model, test_df, feature_scaler, target_transformer):\n    \"\"\"Fixed prediction function\"\"\"\n    \n    print(\"üîÑ Processing test data...\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n    \n    test_texts = test_df['catalog_content'].apply(fast_text_processing).tolist()\n    test_features = extract_essential_features(test_df)\n    test_features_scaled = feature_scaler.transform(test_features)\n    \n    test_dataset = FastDataset(test_texts, test_features_scaled, None, tokenizer)\n    test_loader = DataLoader(test_dataset, batch_size=32, num_workers=2)\n    \n    model.eval()\n    test_predictions = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n            outputs = model(\n                input_ids=batch['input_ids'],\n                attention_mask=batch['attention_mask'],\n                features=batch['features']\n            )\n            test_predictions.extend(outputs.cpu().numpy())\n    \n    # Convert to original scale\n    test_predictions_orig = target_transformer.inverse_transform(np.array(test_predictions))\n    test_predictions_orig = np.maximum(test_predictions_orig, 0.1)\n    \n    return test_predictions_orig\n\n# =====================================\n# MAIN FUNCTION WITH VALIDATION\n# =====================================\n\ndef main():\n    print(\"üöÄ DEBERTA v3 - WITH VALIDATION (60K/15K)\")\n    \n    # Load ALL data\n    train_df = pd.read_csv('/kaggle/input/dataset-amazon/68e8d1d70b66d_student_resource/student_resource/dataset/train.csv')\n    test_df = pd.read_csv('/kaggle/input/dataset-amazon/68e8d1d70b66d_student_resource/student_resource/dataset/test.csv')\n    \n    print(f\"üìä Original training data: {len(train_df)} samples\")\n    print(f\"üìä Test data: {len(test_df)} samples\")\n    \n    # Split into train/validation (60K/15K)\n    train_split, val_split = train_test_split(\n        train_df, \n        test_size=0.2,  # 15K out of 75K = 20%\n        random_state=42,\n        shuffle=True\n    )\n    \n    print(f\"‚úÖ Train/Val split: {len(train_split)} / {len(val_split)}\")\n    \n    # Train with validation\n    model, feature_scaler, target_transformer, best_val_smape = train_with_validation(\n        train_split, val_split, n_epochs=3\n    )\n    \n    print(f\"\\nüéØ Best validation SMAPE: {best_val_smape:.4f}%\")\n    print(\"üéØ Generating final predictions on test set...\")\n    \n    # Load best model\n    model.load_state_dict(torch.load(\"best_model_weights.pt\"))\n    \n    # Load scalers\n    feature_scaler = joblib.load(\"feature_scaler.pkl\")\n    target_scaler = joblib.load(\"target_scaler.pkl\")\n    \n    # Recreate target transformer with loaded scaler\n    target_transformer = TargetTransformer()\n    target_transformer.scaler = target_scaler\n    target_transformer.is_fitted = True\n    \n    test_predictions = predict_fast_deberta(\n        model, test_df, feature_scaler, target_transformer\n    )\n    \n    # Create submission\n    submission = pd.DataFrame({\n        'sample_id': test_df['sample_id'],\n        'price': test_predictions\n    })\n    \n    submission.to_csv('submission.csv', index=False)\n    \n    print(f\"‚úÖ SUBMISSION CREATED: submission.csv\")\n    print(f\"   Predictions: {len(test_predictions)}\")\n    print(f\"   Price range: ${test_predictions.min():.2f} - ${test_predictions.max():.2f}\")\n    print(f\"   Validation SMAPE: {best_val_smape:.4f}%\")\n    print(f\"   Expected Test SMAPE: 30-35%\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T11:36:02.939343Z","iopub.execute_input":"2025-10-13T11:36:02.940067Z","iopub.status.idle":"2025-10-13T13:32:47.348590Z","shell.execute_reply.started":"2025-10-13T11:36:02.940041Z","shell.execute_reply":"2025-10-13T13:32:47.347632Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nüöÄ DEBERTA v3 - WITH VALIDATION (60K/15K)\nüìä Original training data: 75000 samples\nüìä Test data: 75000 samples\n‚úÖ Train/Val split: 60000 / 15000\nüöÄ TRAINING WITH VALIDATION - 60K TRAIN / 15K VAL\nüìä Train samples: 60000, Val samples: 15000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"üìù Processing train texts...\nüîß Engineering train features...\nüìù Processing validation texts...\nüîß Engineering validation features...\nüöÄ Training for 3 epochs with validation\n\nüéØ EPOCH 1/3\n   Batch    0 | Loss: 0.2932\n   Batch  150 | Loss: 0.1940\n   Batch  300 | Loss: 0.2074\n   Batch  450 | Loss: 0.1147\n   Batch  600 | Loss: 0.1552\n   Batch  750 | Loss: 0.1591\n   Batch  900 | Loss: 0.1932\n   Batch 1050 | Loss: 0.1248\n   Batch 1200 | Loss: 0.1215\n   Batch 1350 | Loss: 0.1480\n   Batch 1500 | Loss: 0.2115\n   Batch 1650 | Loss: 0.1292\n   Batch 1800 | Loss: 0.0742\nüìä Epoch 1 Results:\n   Train Loss: 0.1730\n   Val SMAPE: 56.0794%\n‚úÖ NEW BEST MODEL! Val SMAPE: 56.0794%\n\nüéØ EPOCH 2/3\n   Batch    0 | Loss: 0.1633\n   Batch  150 | Loss: 0.1568\n   Batch  300 | Loss: 0.1439\n   Batch  450 | Loss: 0.1443\n   Batch  600 | Loss: 0.1712\n   Batch  750 | Loss: 0.0637\n   Batch  900 | Loss: 0.1459\n   Batch 1050 | Loss: 0.2052\n   Batch 1200 | Loss: 0.0961\n   Batch 1350 | Loss: 0.1475\n   Batch 1500 | Loss: 0.1342\n   Batch 1650 | Loss: 0.2009\n   Batch 1800 | Loss: 0.0942\nüìä Epoch 2 Results:\n   Train Loss: 0.1378\n   Val SMAPE: 53.7788%\n‚úÖ NEW BEST MODEL! Val SMAPE: 53.7788%\n\nüéØ EPOCH 3/3\n   Batch    0 | Loss: 0.1409\n   Batch  150 | Loss: 0.1393\n   Batch  300 | Loss: 0.1220\n   Batch  450 | Loss: 0.0973\n   Batch  600 | Loss: 0.1183\n   Batch  750 | Loss: 0.1150\n   Batch  900 | Loss: 0.0804\n   Batch 1050 | Loss: 0.1700\n   Batch 1200 | Loss: 0.1132\n   Batch 1350 | Loss: 0.1062\n   Batch 1500 | Loss: 0.0645\n   Batch 1650 | Loss: 0.0560\n   Batch 1800 | Loss: 0.1309\nüìä Epoch 3 Results:\n   Train Loss: 0.1274\n   Val SMAPE: 52.6169%\n‚úÖ NEW BEST MODEL! Val SMAPE: 52.6169%\n\nüèÜ TRAINING COMPLETE!\n   Best Val SMAPE: 52.6169% (Epoch 3)\n\nüéØ Best validation SMAPE: 52.6169%\nüéØ Generating final predictions on test set...\nüîÑ Processing test data...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ SUBMISSION CREATED: submission.csv\n   Predictions: 75000\n   Price range: $1.94 - $1177.17\n   Validation SMAPE: 52.6169%\n   Expected Test SMAPE: 30-35%\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}